{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Main Script to Run\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "1. Open a Paper on Zotero\n",
    "2. Copy the paper into a text file named \"paper.txt\". Save this file into a folder named after the author. (ex. Doe et al., 2015)\n",
    "3. Create annotations, extract them into a text file, and save them as \"notes.txt\"\n",
    "4. Drag & drop the folders into the \"_inbox\" folder, in the same parent folder where all the paper folders are stored. The folders in the _inbox folder will be the ones that the script finds and acts on.\n",
    "5. Run the Script \n",
    "6. A set of Anki flashcards will be generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## complete example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for collection: 63.001 Neural Engineering and Signal Processing\n",
      "Collection key: N9F6KIU3\n",
      "Total items pulled: 16\n",
      "    • note 9X4ITAP5 bucketed under TOP WVEM3QEJ\n",
      "    • note YZE9W56W bucketed under TOP NWRNN2L8\n",
      "PARENT WVEM3QEJ: 1 annotation‑notes\n",
      "   ↳ '# Annotations  \\n(3/19/2025, 9:57:50 AM)\\n\\n“One hypothesis sug'\n",
      "PARENT NWRNN2L8: 1 annotation‑notes\n",
      "   ↳ '# Annotations  \\n(11/3/2024, 7:26:25 PM)\\n\\n“Main results. The '\n",
      "[+] Papers with matching notes: 2\n",
      "Papers: 3   Notes: 5\n",
      "Created deck: CMU.Automated Reviews::63.001 Neural Engineering and Signal Processing::Graham et al., 2022\n",
      "Generating cards for Graham 2022  (notes=1)\n",
      "  → 9 cards\n",
      "Created deck: CMU.Automated Reviews::63.001 Neural Engineering and Signal Processing::Dalrymple et al., 2021\n",
      "Generating cards for Dalrymple 2021  (notes=1)\n",
      "  → 4 cards\n"
     ]
    }
   ],
   "source": [
    "import os, html2text, requests\n",
    "from dotenv import load_dotenv\n",
    "from pyzotero import zotero\n",
    "\n",
    "\n",
    "# library I want to use\n",
    "library = \"63.001 Neural Engineering and Signal Processing\"\n",
    "\n",
    "\n",
    "# ─── Config ────────────────────────────────────────────────────────────────\n",
    "load_dotenv()\n",
    "ZOTERO = zotero.Zotero(\n",
    "    os.getenv(\"ZOTERO_USER_ID\"),\n",
    "    os.getenv(\"ZOTERO_LIBRARY_TYPE\", \"user\"),\n",
    "    os.getenv(\"ZOTERO_API_KEY\")\n",
    ")\n",
    "ANKI_URL       = \"http://127.0.0.1:8765\"\n",
    "OPENAI_URL     = \"https://api.openai.com/v1/chat/completions\"\n",
    "OPENAI_KEY     = os.getenv(\"OPENAI_API_KEY\")\n",
    "PARENT_DECK    = f\"CMU.Automated Reviews::{library}\"\n",
    "VERBOSE        = True   # ← flip to False to silence output\n",
    "\n",
    "H2M = html2text.HTML2Text();  H2M.ignore_links = True\n",
    "\n",
    "# ─── Helpers ───────────────────────────────────────────────────────────────\n",
    "def vprint(*msg):\n",
    "    if VERBOSE: print(*msg)\n",
    "\n",
    "def all_collections(limit=100):\n",
    "    out, start = [], 0\n",
    "    while True:\n",
    "        page = ZOTERO.collections(limit=limit, start=start)\n",
    "        out.extend(page)\n",
    "        if len(page) < limit: break\n",
    "        start += limit\n",
    "    return out\n",
    "\n",
    "def fetch_items(coll_key, limit=100):\n",
    "    out, start = [], 0\n",
    "    while True:\n",
    "        page = ZOTERO.collection_items(coll_key, limit=limit, start=start)\n",
    "        out.extend(page)\n",
    "        if len(page) < limit: break\n",
    "        start += limit\n",
    "    return out\n",
    "\n",
    "def existing_decks():\n",
    "    r = requests.post(ANKI_URL, json={\"action\":\"deckNames\",\"version\":6}).json()\n",
    "    return set(r.get(\"result\", []))\n",
    "\n",
    "def ensure_deck(deck):\n",
    "    requests.post(ANKI_URL, json={\n",
    "        \"action\":\"createDeck\",\"version\":6,\"params\":{\"deck\":deck}\n",
    "    })\n",
    "\n",
    "def push_card(deck, front, back):\n",
    "    requests.post(ANKI_URL, json={\n",
    "        \"action\":\"addNotes\",\"version\":6,\n",
    "        \"params\":{\"notes\":[{\n",
    "            \"deckName\":deck,\"modelName\":\"Basic\",\n",
    "            \"fields\":{\"Front\":front,\"Back\":back},\n",
    "            \"tags\":[\"paper\",\"notecard\"]\n",
    "        }]}\n",
    "    })\n",
    "\n",
    "def generate_cards(text):\n",
    "    hdr = {\"Content-Type\":\"application/json\",\"Authorization\":f\"Bearer {OPENAI_KEY}\"}\n",
    "    data = {\n",
    "        \"model\":\"gpt-3.5-turbo\",\n",
    "        \"temperature\":0.7,\n",
    "        \"messages\":[\n",
    "            {\"role\":\"system\",\"content\":(\n",
    "                \"You are given annotations from a research paper. \"\n",
    "                \"Create concise Q/A notecards (≤15). Each card starts with Q:/A: \"\n",
    "                \"Include paper reference in the question.\")\n",
    "            },\n",
    "            {\"role\":\"user\",\"content\":text}\n",
    "        ]\n",
    "    }\n",
    "    r = requests.post(OPENAI_URL, headers=hdr, json=data).json()\n",
    "    return r[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "def parse_cards(txt):\n",
    "    q, a, out = \"\", \"\", []\n",
    "    for ln in txt.splitlines():\n",
    "        ln = ln.strip()\n",
    "        if ln.startswith(\"Q:\"):\n",
    "            if q and a: out.append((q,a))\n",
    "            q, a = ln[2:].strip(), \"\"\n",
    "        elif ln.startswith(\"A:\"):\n",
    "            a = ln[2:].strip()\n",
    "    if q and a: out.append((q,a))\n",
    "    return out\n",
    "\n",
    "# ─── Main ───────────────────────────────────────────────────────────────────\n",
    "def run(collection_name):\n",
    "    vprint(f\"Looking for collection: {collection_name}\")\n",
    "    coll_key = next(\n",
    "        c[\"data\"][\"key\"] for c in all_collections()\n",
    "        if c[\"data\"][\"name\"] == collection_name\n",
    "    )\n",
    "    vprint(\"Collection key:\", coll_key)\n",
    "\n",
    "    items = fetch_items(coll_key)\n",
    "    vprint(f\"Total items pulled: {len(items)}\")\n",
    "\n",
    "    # split items\n",
    "    notes = [i for i in items if i[\"data\"][\"itemType\"]==\"note\"]\n",
    "\n",
    "    # index every pulled item by key so we can walk parent links fast\n",
    "    items_by_key = {i[\"key\"]: i for i in items}\n",
    "\n",
    "    def top_level_key(k):\n",
    "        \"\"\"Follow parentItem links until we reach a top‑level item.\"\"\"\n",
    "        while True:\n",
    "            itm = items_by_key.get(k)\n",
    "            parent = itm and itm[\"data\"].get(\"parentItem\")\n",
    "            if not parent:\n",
    "                return k          # k is now a top‑level item\n",
    "            k = parent            # climb one level\n",
    "\n",
    "    annos = {}\n",
    "    for n in notes:\n",
    "        md = H2M.handle(n[\"data\"][\"note\"]).strip()\n",
    "        head = md.lower()[:80]\n",
    "        if \"annotations\" not in head:\n",
    "            continue\n",
    "\n",
    "        top_key = top_level_key(n[\"data\"][\"parentItem\"])\n",
    "        annos.setdefault(top_key, []).append(md)\n",
    "        vprint(f\"    • note {n['key']} bucketed under TOP {top_key}\")\n",
    "\n",
    "    # troubleshooting: used to make sure the right notes are captured\n",
    "    for pid, txts in annos.items():\n",
    "        print(f\"PARENT {pid}: {len(txts)} annotation‑notes\")\n",
    "        # If you want to see the first 60 chars of each note:\n",
    "        for t in txts:\n",
    "            print(\"   ↳\", repr(t[:60]))\n",
    "\n",
    "    print(f\"[+] Papers with matching notes: {len(annos)}\")\n",
    "\n",
    "\n",
    "\n",
    "    papers_by_id = {\n",
    "        i[\"key\"]: i for i in items\n",
    "        if i[\"data\"][\"itemType\"] in {\"journalArticle\",\"conferencePaper\",\"report\"}\n",
    "    }\n",
    "    vprint(f\"Papers: {len(papers_by_id)}   Notes: {len(notes)}\")\n",
    "\n",
    "    decks_exist = existing_decks()\n",
    "\n",
    "    for pid, txts in annos.items():\n",
    "        paper = papers_by_id.get(pid)\n",
    "        if not paper:\n",
    "            vprint(\"Orphan note, skipping:\", pid);  continue\n",
    "\n",
    "        creator = paper[\"data\"][\"creators\"][0]\n",
    "        author  = creator.get(\"lastName\",\"Unknown\")\n",
    "        year    = paper[\"data\"].get(\"date\",\"\")[:4] or \"n.d.\"\n",
    "        deck    = f\"{PARENT_DECK}::{author} et al., {year}\"\n",
    "\n",
    "        if deck not in decks_exist:\n",
    "            ensure_deck(deck); decks_exist.add(deck); vprint(\"Created deck:\", deck)\n",
    "\n",
    "        notes_block = \"\\n\\n\".join(txts)\n",
    "        vprint(f\"Generating cards for {author} {year}  (notes={len(txts)})\")\n",
    "        cards = parse_cards(generate_cards(notes_block))\n",
    "        vprint(f\"  → {len(cards)} cards\")\n",
    "\n",
    "        for q,a in cards:\n",
    "            push_card(deck,q,a)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run(library)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
